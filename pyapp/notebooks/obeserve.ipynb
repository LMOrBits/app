{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May 8, 2025 > 11:55:03 |  pyapp.observation.phoneix : 12 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Starting Phoenix observation \u001b[0m \n",
      "May 8, 2025 > 11:55:03 |  taskpy.main : 28 |  \u001b[34m\u001b[1m üêû DEBUG  |  ['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/src/pyapp/observation'), 'status'] \u001b[0m \n",
      "May 8, 2025 > 11:55:04 |  taskpy.main : 32 |  \u001b[31m\u001b[1m ‚ùå ERROR  |  \u001b[32mtask: [status] docker ps | grep lmorbits-obeservation | wc -l\n",
      "\u001b[0m \u001b[0m \n",
      "May 8, 2025 > 11:55:04 |  taskpy.main : 34 |  \u001b[1m ‚ÑπÔ∏è INFO  |         1\n",
      " \u001b[0m \n",
      "May 8, 2025 > 11:55:04 |  pyapp.observation.instance : 34 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Phoenix is already running \u001b[0m \n",
      "True\n",
      "May 8, 2025 > 11:55:04 |  pyapp.serve_integration.mlflow_llamacpp : 23 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Config path: /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/model_dir \u001b[0m \n",
      "May 8, 2025 > 11:55:04 |  pyapp.serve_integration.mlflow_llamacpp : 24 |  \u001b[1m ‚ÑπÔ∏è INFO  |  MLflow client: <mlflow.tracking.client.MlflowClient object at 0x1093475f0> \u001b[0m \n",
      "May 8, 2025 > 11:55:04 |  pyapp.serve_integration.mlflow_llamacpp : 26 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Experiments: [<Experiment: artifact_location='mlflow-artifacts:/0', creation_time=1745927919661, experiment_id='0', last_update_time=1745927919661, lifecycle_stage='active', name='Default', tags={}>] \u001b[0m \n",
      "May 8, 2025 > 11:55:04 |  serve.servers.llamacpp.serve : 54 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized LLaMA CPP server manager \u001b[0m \n",
      "May 8, 2025 > 11:55:04 |  serve.utils.model_config : 66 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized model configuration manager with config path: /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/model_dir \u001b[0m \n",
      "May 8, 2025 > 11:55:04 |  serve.utils.mlflow.config : 127 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized MLflow model configuration manager \u001b[0m \n",
      "May 8, 2025 > 11:55:04 |  serve.experiment_tracker.mlflow.mlflow_llamacpp.manager : 88 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized model manager successfully \u001b[0m \n",
      "May 8, 2025 > 11:55:04 |  pyapp.model_connection.lm.langchain.litellm : 17 |  \u001b[1m ‚ÑπÔ∏è INFO  |  connecing to mlflow ... \u001b[0m \n",
      "May 8, 2025 > 11:55:04 |  serve.servers.llamacpp.serve : 164 |  \u001b[31m\u001b[1m ‚ùå ERROR  |  Failed to get status for model rag_model: Model ID 'rag_model' not found \u001b[0m \n",
      "May 8, 2025 > 11:55:04 |  serve._cli.task : 29 |  \u001b[34m\u001b[1m üêû DEBUG  |  ['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/serve/src/serve/servers/llamacpp'), 'serve', 'MODEL_PATH=/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/models/rag_model/model_path/artifacts', 'PORT=8080', 'MODEL_NAME=model.gguf', 'MODEL_ID=rag_model'] \u001b[0m \n",
      "May 8, 2025 > 11:55:05 |  serve._cli.task : 33 |  \u001b[31m\u001b[1m ‚ùå ERROR  |  \u001b[32mtask: [serve] echo \"Starting LlamaCpp server and waiting for it to be ready...\"\n",
      "\u001b[0m\u001b[32mtask: [serve] container_id=$(docker run -d \\\n",
      "  --name lmorbits-llamacpp-rag_model \\\n",
      "  -p 8080:8080 \\\n",
      "  -v /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/models/rag_model/model_path/artifacts:/models \\\n",
      "  ghcr.io/ggerganov/llama.cpp:server \\\n",
      "  -m /models/model.gguf)\n",
      "\n",
      "while ! docker logs $container_id 2>&1 | grep -q \"GET /health 127.0.0.1 200\"; do\n",
      "  echo \"Loading model and starting server...\"\n",
      "  docker logs --tail 10 $container_id\n",
      "  sleep 1\n",
      "done\n",
      "echo \"LlamaCpp server is ready!\"\n",
      "\n",
      "\u001b[0mdocker: Error response from daemon: Conflict. The container name \"/lmorbits-llamacpp-rag_model\" is already in use by container \"1c417611e9797bbae3868a17c1a7ea12d0d9a3529ea090c98d679ee3f4cf5075\". You have to remove (or rename) that container to be able to reuse that name.\n",
      "See 'docker run --help'.\n",
      "\u001b[31mtask: Failed to run task \"serve\": exit status 125\n",
      "\u001b[0m \u001b[0m \n",
      "May 8, 2025 > 11:55:05 |  serve._cli.task : 35 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Starting LlamaCpp server and waiting for it to be ready...\n",
      " \u001b[0m \n",
      "May 8, 2025 > 11:55:05 |  serve.servers.llamacpp.serve : 140 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Started server for model ID: rag_model \u001b[0m \n",
      "May 8, 2025 > 11:55:05 |  serve.servers.llamacpp.serve : 91 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Added new server instance with model ID: rag_model \u001b[0m \n",
      "May 8, 2025 > 11:55:05 |  serve.experiment_tracker.mlflow.mlflow_llamacpp.manager : 198 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Started serving model rag_model on ports 8080 \u001b[0m \n"
     ]
    }
   ],
   "source": [
    "from pyapp.observation.phoneix import traced_agent,ph_instrumentor\n",
    "\n",
    "# -------------------model -------------------#\n",
    "from pyapp.model_connection.lm.langchain.litellm import get_model_mlflow_llamacpp, ModelConfig\n",
    "### --- config --- ###\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "here = Path(\"./\").resolve()\n",
    "print(Path(here / \"config.env\").exists())\n",
    "load_dotenv(here / \"config.env\")\n",
    "\n",
    "mlflow_port = os.getenv(\"MLFLOW_PORT\") or \"5001\"\n",
    "tracking_uri=f\"http://localhost:{mlflow_port}\"\n",
    "config_path=here / \"model_dir\"\n",
    "\n",
    "### --- model --- ###\n",
    "model_config = ModelConfig(model_name=\"rag_model\",port=8080,gguf_relative_path=\"model_path/artifacts/model.gguf\")\n",
    "model = get_model_mlflow_llamacpp(tracking_uri,config_path,model_config , stream=True , mock_response=\"Hello world\", temperature=0, max_tokens=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------traced_agent -------------------#\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "@traced_agent(name=\"app1\")\n",
    "def assistant(messages: list[dict], session_id: str):\n",
    "    # now you only have to do your business logic\n",
    "    return chain.invoke(messages)\n",
    "\n",
    "@traced_agent(name=\"app2\")\n",
    "def assistant2(messages: list[dict], session_id: str):\n",
    "    # now you only have to do your business logic\n",
    "    return chain.invoke(messages)\n",
    "\n",
    "\n",
    "@traced_agent(name=\"app3\")\n",
    "def assistant3(messages: list[dict], session_id: str):\n",
    "    answer, trace_url = assistant(messages, session_id)\n",
    "    answer2, trace_url2 = assistant2(messages, session_id)\n",
    "    return chain.invoke(messages)\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": \"hi! im bob\"}\n",
    "]\n",
    "import uuid\n",
    "session_id = str(uuid.uuid4())\n",
    "answer, trace_url = assistant3(messages, session_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://localhost:6006/projects/UHJvamVjdDoy/traces/0c6a338e98f14f38b55b7e4805bf3c89?selectedSpanNodeId=U3Bhbjoy'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://localhost:6006/projects/UHJvamVjdDoy/traces/8a35fee6d45b2f15802acc9ea53b157b?selectedSpanNodeId=U3Bhbjoy\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from openinference.instrumentation import using_session\n",
    "from openinference.semconv.trace import SpanAttributes\n",
    "from opentelemetry import trace # wherever your context manager lives\n",
    "from functools import wraps\n",
    "\n",
    "import base64\n",
    "def traced_agent(name: str, propagate_session: bool = True, tracer_name:str=\"lmorbits-trace\"):\n",
    "    \"\"\"\n",
    "    Decorator that wraps a function in a span named `name`,\n",
    "    automatically sets SESSION_ID, INPUT_VALUE, and OUTPUT_VALUE,\n",
    "    and (optionally) enters using_session(session_id) around the call.\n",
    "    \"\"\"\n",
    "    tracer = trace.get_tracer(tracer_name)\n",
    "    def decorator(fn):\n",
    "        @wraps(fn)\n",
    "        def wrapper(messages: list[dict], session_id: str, *args, **kwargs):\n",
    "            # start the OpenTelemetry span\n",
    "            with tracer.start_as_current_span(\n",
    "                name=name,\n",
    "                attributes={SpanAttributes.OPENINFERENCE_SPAN_KIND: \"agent\"}\n",
    "            ) as span:\n",
    "                # record session and input\n",
    "                trace_id = span.get_span_context().trace_id\n",
    "                span_id = span.get_span_context().index(0)\n",
    "                text = f\"Span:{span_id}\"\n",
    "                st_big = base64.b64encode(text.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "                trace_id_hex = format(trace_id, \"032x\")  # Converts to 32-character hex\n",
    "                \n",
    "                trace_url = f\"{ph_instrumentor.project_url}/traces/{trace_id_hex}?selectedSpanNodeId={st_big}\"\n",
    "                \n",
    "                span.set_attribute(SpanAttributes.SESSION_ID, session_id)\n",
    "                last_msg = messages[-1].get(\"content\")\n",
    "                span.set_attribute(SpanAttributes.INPUT_VALUE, last_msg)\n",
    "\n",
    "                # optionally propagate session into sub‚Äêspans\n",
    "                if propagate_session:\n",
    "                    with using_session(session_id):\n",
    "                        result = fn(messages, session_id, *args, **kwargs)\n",
    "                else:\n",
    "                    result = fn(messages, session_id, *args, **kwargs)\n",
    "\n",
    "                # record the output\n",
    "                # assume returned object has .content\n",
    "                output = getattr(result, \"content\", result)\n",
    "                span.set_attribute(SpanAttributes.OUTPUT_VALUE, output)\n",
    "\n",
    "                return result,trace_url\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "@traced_agent(name=\"app1\")\n",
    "def assistant(messages: list[dict], session_id: str):\n",
    "    # now you only have to do your business logic\n",
    "    return chain.invoke(messages)\n",
    "\n",
    "@traced_agent(name=\"app2\")\n",
    "def assistant2(messages: list[dict], session_id: str):\n",
    "    # now you only have to do your business logic\n",
    "    return chain.invoke(messages)\n",
    "\n",
    "\n",
    "@traced_agent(name=\"app3\")\n",
    "def assistant3(messages: list[dict], session_id: str):\n",
    "    answer, trace_url = assistant(messages, session_id)\n",
    "    answer2, trace_url2 = assistant2(messages, session_id)\n",
    "    return chain.invoke(messages)\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": \"hi! im bob\"}\n",
    "]\n",
    "import uuid\n",
    "session_id = str(uuid.uuid4())\n",
    "\n",
    "\n",
    "answer, trace_url = assistant3(messages, session_id)\n",
    "\n",
    "print(trace_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"[{'role': 'user', 'content': 'hi! im bob'}]\\n\\n```\", additional_kwargs={}, response_metadata={'token_usage': Usage(completion_tokens=20, prompt_tokens=37, total_tokens=57, completion_tokens_details=None, prompt_tokens_details=None), 'model': 'openai/custom', 'finish_reason': 'length', 'model_name': 'openai/custom'}, id='run--db61f2ab-b8b2-478a-a49b-806a2ebee493-0', usage_metadata={'input_tokens': 37, 'output_tokens': 20, 'total_tokens': 57})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": \"hi! im bob\"}\n",
    "]\n",
    "session_id = str(uuid.uuid4())\n",
    "assistant2(messages, session_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyapp.observation.phoneix import observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U3Bhbjoy\n",
      "2\n",
      "http://localhost:6006/projects/UHJvamVjdDoy/traces/ca96755380cff2806a7c11695eb119b6?selectedSpanNodeId=U3Bhbjoy\n"
     ]
    }
   ],
   "source": [
    "from openinference.instrumentation.helpers import get_span_id\n",
    "import uuid\n",
    "from opentelemetry import trace\n",
    "import base64\n",
    "tracer = trace.get_tracer(\"ap\")\n",
    "session_id = str(uuid.uuid4())\n",
    "with tracer.start_as_current_span(\"app_test\") as span:\n",
    "    trace_id = span.get_span_context().trace_id\n",
    "    span_id = span.get_span_context().index(0)\n",
    "    text = f\"Span:{span_id}\"\n",
    "    st_big = base64.b64encode(text.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "    print(st_big)\n",
    "    trace_id_hex = format(trace_id, \"032x\")  # Converts to 32-character hex\n",
    "    print(span_id)\n",
    "    print(f\"{ph_instrumentor.project_url}/traces/{trace_id_hex}?selectedSpanNodeId={st_big}\")\n",
    "    span.set_attribute(\"session_id\", session_id)\n",
    "    span.set_attribute(\"input_value\", \"test\")\n",
    "    answer = \"hi there\"\n",
    "    span.set_attribute(\"output_value\", answer)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:6006/projects/UHJvamVjdDoy/traces/27baaf9284f28b54c94f169a3d1cc2b1?selectedSpanNodeId=U3Bhbjoz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "U3BhbjozNQ=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'00000000000000006aa1110c0a488759'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = \"U3BhbjozNQ==\" \n",
    "num = \"7683441182680581977\"\n",
    "format(int(num), \"032x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Span:35\n",
      "U3BhbjozNQ==\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "# --- decode the given st back to text ---\n",
    "st = \"U3BhbjozNQ==\"\n",
    "decoded = base64.b64decode(st).decode(\"utf-8\")\n",
    "print(decoded)  \n",
    "# ‚Üí Span:35\n",
    "\n",
    "\n",
    "# --- now encode a (possibly new) num to the same format ---\n",
    "num = \"35\"                  # the number you want to package\n",
    "text = f\"Span:{num}\"        # prefix it with \"Span:\"\n",
    "st_new = base64.b64encode(text.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "print(st_new)\n",
    "# ‚Üí U3BhbjozNQ=="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U3Bhbjo3NjgzNDQxMTgyNjgwNTgxOTc3\n"
     ]
    }
   ],
   "source": [
    "num = \"7683441182680581977\"\n",
    "text = f\"Span:{num}\"\n",
    "st_big = base64.b64encode(text.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "print(st_big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U3Bhbjo3NjgzNDQxMTgyNjgwNTgxOTc3\n",
    "\n",
    "import requests\n",
    "\n",
    "response = requests.get(\n",
    "    f\"http://localhost:6006/v1/projects/{a.project_name}\",\n",
    "    headers={\"Accept\":\"*/*\"},\n",
    ")\n",
    "\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UHJvamVjdDoy'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"data\"][\"id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kSdIZKGDVRc=\n"
     ]
    }
   ],
   "source": [
    "number = 10459408256634934551\n",
    "s = number.to_bytes((number.bit_length() + 7) // 8, 'big')  # Convert int to bytes\n",
    "import base64\n",
    "encoded = base64.b64encode(s).decode()\n",
    "print(encoded)  # 'U3Bhbjo0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<opentelemetry.sdk.trace.Tracer at 0x1686b0a40>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.project_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "May 7, 2025 > 17:03:09 |  pyapp.serve_integration.mlflow_llamacpp : 23 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Config path: /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/model_dir \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  pyapp.serve_integration.mlflow_llamacpp : 24 |  \u001b[1m ‚ÑπÔ∏è INFO  |  MLflow client: <mlflow.tracking.client.MlflowClient object at 0x10925efc0> \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  pyapp.serve_integration.mlflow_llamacpp : 26 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Experiments: [<Experiment: artifact_location='mlflow-artifacts:/0', creation_time=1745927919661, experiment_id='0', last_update_time=1745927919661, lifecycle_stage='active', name='Default', tags={}>] \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  serve.servers.llamacpp.serve : 54 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized LLaMA CPP server manager \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  serve.utils.model_config : 66 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized model configuration manager with config path: /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/model_dir \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  serve.utils.mlflow.config : 127 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized MLflow model configuration manager \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  serve.experiment_tracker.mlflow.mlflow_llamacpp.manager : 88 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized model manager successfully \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  pyapp.model_connection.lm.langchain.litellm : 17 |  \u001b[1m ‚ÑπÔ∏è INFO  |  connecing to mlflow ... \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  serve.servers.llamacpp.serve : 164 |  \u001b[31m\u001b[1m ‚ùå ERROR  |  Failed to get status for model rag_model: Model ID 'rag_model' not found \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  serve._cli.task : 29 |  \u001b[34m\u001b[1m üêû DEBUG  |  ['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/serve/src/serve/servers/llamacpp'), 'serve', 'MODEL_PATH=/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/models/rag_model/model_path/artifacts', 'PORT=8080', 'MODEL_NAME=model.gguf', 'MODEL_ID=rag_model'] \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  serve._cli.task : 33 |  \u001b[31m\u001b[1m ‚ùå ERROR  |  \u001b[32mtask: [serve] echo \"Starting LlamaCpp server and waiting for it to be ready...\"\n",
      "\u001b[0m\u001b[32mtask: [serve] container_id=$(docker run -d \\\n",
      "  --name lmorbits-llamacpp-rag_model \\\n",
      "  -p 8080:8080 \\\n",
      "  -v /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/models/rag_model/model_path/artifacts:/models \\\n",
      "  ghcr.io/ggerganov/llama.cpp:server \\\n",
      "  -m /models/model.gguf)\n",
      "\n",
      "while ! docker logs $container_id 2>&1 | grep -q \"GET /health 127.0.0.1 200\"; do\n",
      "  echo \"Loading model and starting server...\"\n",
      "  docker logs --tail 10 $container_id\n",
      "  sleep 1\n",
      "done\n",
      "echo \"LlamaCpp server is ready!\"\n",
      "\n",
      "\u001b[0mdocker: Error response from daemon: Conflict. The container name \"/lmorbits-llamacpp-rag_model\" is already in use by container \"c7dc2b3387c4e61802e943b8b33baa7d1a678c8b6679041c7560754d869a9ed6\". You have to remove (or rename) that container to be able to reuse that name.\n",
      "See 'docker run --help'.\n",
      "\u001b[31mtask: Failed to run task \"serve\": exit status 125\n",
      "\u001b[0m \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  serve._cli.task : 35 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Starting LlamaCpp server and waiting for it to be ready...\n",
      " \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  serve.servers.llamacpp.serve : 140 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Started server for model ID: rag_model \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  serve.servers.llamacpp.serve : 91 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Added new server instance with model ID: rag_model \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  serve.experiment_tracker.mlflow.mlflow_llamacpp.manager : 198 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Started serving model rag_model on ports 8080 \u001b[0m \n"
     ]
    }
   ],
   "source": [
    "from pyapp.model_connection.lm.langchain.litellm import get_model_mlflow_llamacpp, ModelConfig\n",
    "### --- config --- ###\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "here = Path(\"./\").resolve()\n",
    "print(Path(here / \"config.env\").exists())\n",
    "load_dotenv(here / \"config.env\")\n",
    "\n",
    "mlflow_port = os.getenv(\"MLFLOW_PORT\") or \"5001\"\n",
    "tracking_uri=f\"http://localhost:{mlflow_port}\"\n",
    "config_path=here / \"model_dir\"\n",
    "\n",
    "### --- model --- ###\n",
    "model_config = ModelConfig(model_name=\"rag_model\",port=8080,gguf_relative_path=\"model_path/artifacts/model.gguf\")\n",
    "model = get_model_mlflow_llamacpp(tracking_uri,config_path,model_config , stream=True , mock_response=\"Hello world\", temperature=0, max_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to instrument while already instrumented\n",
      "Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "from opentelemetry.trace import SpanKind\n",
    "from pyapp.observation.phoneix import PhonexLangChainInstrumentor\n",
    "observation = PhonexLangChainInstrumentor(\"lmorbits-phoenix\",\"app2\")\n",
    "tracer = observation.get_tracer(\"__name__\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to instrument while already instrumented\n"
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "from taskpy import TaskCLI\n",
    "from pathlib import Path\n",
    "tracer = None\n",
    "\n",
    "class PhoenixObservation:\n",
    "  @property\n",
    "  def port(self):\n",
    "    return 6006\n",
    "\n",
    "  @property\n",
    "  def grpc_port(self):\n",
    "    return 4317\n",
    "\n",
    "  def __init__(self):\n",
    "    self.task = TaskCLI(Path(__file__).parent )\n",
    "  \n",
    "  \n",
    "  def is_running(self):\n",
    "      try:\n",
    "        answer = self.task.run(\"status\")\n",
    "        return int(answer.stdout.strip())!= 0\n",
    "      except Exception as e:\n",
    "        logger.error(f\"Error in phoenix observation: {e}\")\n",
    "        return False \n",
    "\n",
    "  def start(self):\n",
    "    try:\n",
    "      status = self.is_running()\n",
    "      if status:\n",
    "        logger.info(\"Phoenix is already running\")\n",
    "      else: \n",
    "        self.task.run(\"start\", port=self.port, grpc_port=self.grpc_port)\n",
    "    except Exception as e:\n",
    "      logger.error(f\"Error in phoenix observation: {e}\")\n",
    "\n",
    "  def stop(self):\n",
    "    try:\n",
    "      status = self.is_running()\n",
    "      if not status:\n",
    "        logger.info(\"Phoenix is not running\")\n",
    "      else:\n",
    "        self.task.run(\"stop\")\n",
    "    except Exception as e:\n",
    "      logger.error(f\"Error in phoenix observation: {e}\")\n",
    "  \n",
    "  def remove(self):\n",
    "    try:\n",
    "      self.task.run(\"remove\")\n",
    "    except Exception as e:\n",
    "      logger.error(f\"Error in phoenix observation: {e}\")\n",
    "\n",
    "\n",
    "class PhonexLangChainInstrumentor:\n",
    "  tracer_provider = None\n",
    "\n",
    "  def __init__(self, project_name: str= \"lmorbits-phoenix\" , app_name: str= \"app1\"):\n",
    "    try:\n",
    "      self.app_name = app_name\n",
    "      self.project_name = project_name\n",
    "      from opentelemetry import trace\n",
    "      from openinference.instrumentation.langchain import LangChainInstrumentor\n",
    "      from phoenix.otel import register\n",
    "\n",
    "      self.tracer_provider = register(\n",
    "        project_name=self.project_name,\n",
    "        set_global_tracer_provider=False,\n",
    "        verbose=False\n",
    "      ) \n",
    "      LangChainInstrumentor().instrument(tracer_provider=self.tracer_provider)\n",
    "      trace.set_tracer_provider(self.tracer_provider)\n",
    "      \n",
    "\n",
    "    except Exception as e:\n",
    "      logger.error(f\"Error in phoenix observation: {e}\")\n",
    "  \n",
    "  def get_tracer(self , module_name:str):\n",
    "    from opentelemetry import trace\n",
    "    return trace.get_tracer(f\"{self.app_name}.{module_name}\")\n",
    "    \n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation import using_metadata\n",
    "metadata = {\n",
    "    \"key-1\": \"value_1\",\n",
    "    \"key-2\": \"value_2\",\n",
    "}\n",
    "\n",
    "@using_metadata(metadata)\n",
    "def call_fn(*args, **kwargs):\n",
    "    print(\"call_fn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation import TraceConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.arize.com/phoenix/tracing/how-to-tracing/setup-tracing/setup-sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SpanKind' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m answer\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# tracer2 = trace.get_tracer(\"2\")\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# @tracer2.start_as_current_span(f\"app2\",kind=SpanKind.INTERNAL, attributes={SpanAttributes.OPENINFERENCE_SPAN_KIND: \"agent\"})\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;66;03m# async def app2(message:str, session_id:str):\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m \u001b[38;5;66;03m#     prompt_template_variables=prompt_template_variables,\u001b[39;00m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# ):\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tracer.start_as_current_span(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m,kind=\u001b[43mSpanKind\u001b[49m.CLIENT,) \u001b[38;5;28;01mas\u001b[39;00m main_span:\n\u001b[32m     53\u001b[39m     session_id = \u001b[38;5;28mstr\u001b[39m(uuid.uuid4())\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m app1(\u001b[33m\"\u001b[39m\u001b[33mhello\u001b[39m\u001b[33m\"\u001b[39m, session_id)\n",
      "\u001b[31mNameError\u001b[39m: name 'SpanKind' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from openinference.instrumentation import using_attributes, using_user , using_metadata, using_session\n",
    "from openinference.semconv.trace import SpanAttributes\n",
    "import uuid\n",
    "from opentelemetry import trace\n",
    "\n",
    "tracer = trace.get_tracer(\"1\")\n",
    "\n",
    "\n",
    "async def app1(message:str, session_id:str):\n",
    "\n",
    "    with tracer.start_as_current_span(f\"app1\",kind=SpanKind.INTERNAL) as app1_span:\n",
    "        app1_span.set_attribute(SpanAttributes.SESSION_ID, session_id)\n",
    "        app1_span.set_attribute(SpanAttributes.INPUT_VALUE, message)\n",
    "        with using_session(session_id):\n",
    "            answer = \"\"\n",
    "            async for chunk in model.astream(message):\n",
    "                answer += chunk.content\n",
    "                print(chunk, end=\"|\", flush=True)\n",
    "    \n",
    "        app1_span.set_attribute(SpanAttributes.OUTPUT_VALUE, answer)\n",
    "    return answer\n",
    "\n",
    "# tracer2 = trace.get_tracer(\"2\")\n",
    "# @tracer2.start_as_current_span(f\"app2\",kind=SpanKind.INTERNAL, attributes={SpanAttributes.OPENINFERENCE_SPAN_KIND: \"agent\"})\n",
    "# async def app2(message:str, session_id:str):\n",
    "#     current_span = trace.get_current_span()\n",
    "#     current_span.set_attribute(SpanAttributes.SESSION_ID, session_id)\n",
    "#     current_span.set_attribute(SpanAttributes.INPUT_VALUE, message)\n",
    "#     with using_session(session_id):\n",
    "#         answer = \"\"\n",
    "#         async for chunk in model.astream(message):\n",
    "#             answer += chunk\n",
    "#             print(chunk, end=\"|\", flush=True)\n",
    "    \n",
    "#     current_span.set_attribute(SpanAttributes.OUTPUT_VALUE, answer)\n",
    "#     return answer\n",
    "\n",
    "# prompt_template = \"Please describe the weather forecast for {city} on {date}\"\n",
    "# prompt_template_variables = {\"city\": \"Johannesburg\", \"date\":\"July 11\"}\n",
    "# prompt_template_version = \"v1.0\"\n",
    "# with using_attributes(\n",
    "#     session_id=\"my-session-id\",\n",
    "#     user_id=\"my-user-id\",\n",
    "#     metadata=metadata,\n",
    "#     tags=tags,\n",
    "#     prompt_template=prompt_template,\n",
    "#     prompt_template_version=prompt_template_version,\n",
    "#     prompt_template_variables=prompt_template_variables,\n",
    "# ):\n",
    "    \n",
    "\n",
    "with tracer.start_as_current_span(f\"test\",kind=SpanKind.CLIENT,) as main_span:\n",
    "    session_id = str(uuid.uuid4())\n",
    "    await app1(\"hello\", session_id)\n",
    "# app2(\"i\", session_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746632636.184881  568758 chttp2_transport.cc:1201] ipv6:%5B::1%5D:4317: Got goaway [11] err=UNAVAILABLE:GOAWAY received; Error code: 11; Debug Text: ping_timeout {grpc_status:14, http2_error:11, created_time:\"2025-05-07T18:43:56.184875+03:00\"}\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "import openai\n",
    "from openinference.instrumentation import using_session\n",
    "from openinference.semconv.trace import SpanAttributes\n",
    "from opentelemetry import trace\n",
    "\n",
    "session_id = str(uuid.uuid4())\n",
    "\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "@tracer.start_as_current_span(name=\"app1\", attributes={SpanAttributes.OPENINFERENCE_SPAN_KIND: \"agent\"})\n",
    "def assistant(\n",
    "  messages: list[dict],\n",
    "  session_id: str = str,\n",
    "):\n",
    "  current_span = trace.get_current_span()\n",
    "  current_span.set_attribute(SpanAttributes.SESSION_ID, session_id)\n",
    "  current_span.set_attribute(SpanAttributes.INPUT_VALUE, messages[-1].get('content'))\n",
    "\n",
    "  # Propagate the session_id down to spans crated by the OpenAI instrumentation\n",
    "  # This is not strictly necessary, but it helps to correlate the spans to the same session\n",
    "  with using_session(session_id):\n",
    "     response = chain.invoke(messages)\n",
    "     \n",
    "\n",
    "  current_span.set_attribute(SpanAttributes.OUTPUT_VALUE, response.content)\n",
    "  return response\n",
    "\n",
    "@tracer.start_as_current_span(name=\"app2\", attributes={SpanAttributes.OPENINFERENCE_SPAN_KIND: \"agent\"})\n",
    "def assistant2(\n",
    "  messages: list[dict],\n",
    "  session_id: str = str,\n",
    "):\n",
    "  current_span = trace.get_current_span()\n",
    "  current_span.set_attribute(SpanAttributes.SESSION_ID, session_id)\n",
    "  current_span.set_attribute(SpanAttributes.INPUT_VALUE, messages[-1].get('content'))\n",
    "\n",
    "  # Propagate the session_id down to spans crated by the OpenAI instrumentation\n",
    "  # This is not strictly necessary, but it helps to correlate the spans to the same session\n",
    "  with using_session(session_id):\n",
    "     r = chain.invoke(messages)\n",
    "     \n",
    "\n",
    "  current_span.set_attribute(SpanAttributes.OUTPUT_VALUE, r.content)\n",
    "  return r\n",
    "\n",
    "\n",
    "@tracer.start_as_current_span(name=\"app3\", attributes={SpanAttributes.OPENINFERENCE_SPAN_KIND: \"agent\"})\n",
    "def assistant3(\n",
    "  messages: list[dict],\n",
    "  session_id: str = str,\n",
    "):\n",
    "  current_span = trace.get_current_span()\n",
    "  current_span.set_attribute(SpanAttributes.SESSION_ID, session_id)\n",
    "  current_span.set_attribute(SpanAttributes.INPUT_VALUE, messages[-1].get('content'))\n",
    "\n",
    "  with using_session(session_id):\n",
    "    answer = assistant( messages, session_id)\n",
    "    answer2 = assistant2(messages, session_id)\n",
    "\n",
    "    r = chain.invoke(messages)\n",
    "\n",
    "\n",
    "  current_span.set_attribute(SpanAttributes.OUTPUT_VALUE, r.content)\n",
    "  return r\n",
    "\n",
    "\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": \"hi! im bob\"}\n",
    "]\n",
    "response = assistant3(\n",
    "  messages,\n",
    "  session_id=session_id,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     65\u001b[39m messages = [\n\u001b[32m     66\u001b[39m   {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mhi! im bob\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m     67\u001b[39m ]\n\u001b[32m     68\u001b[39m session_id = \u001b[38;5;28mstr\u001b[39m(uuid.uuid4())\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m answer, trace_url = \u001b[43massistant3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[38;5;28mprint\u001b[39m(trace_url)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mtraced_agent.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(messages, session_id, *args, **kwargs)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m propagate_session:\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m using_session(session_id):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     37\u001b[39m     result = fn(messages, session_id, *args, **kwargs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36massistant3\u001b[39m\u001b[34m(messages, session_id)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;129m@traced_agent\u001b[39m(name=\u001b[33m\"\u001b[39m\u001b[33mapp3\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34massistant3\u001b[39m(messages: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m], session_id: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     answer, trace_url = \u001b[43massistant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m     answer2, trace_url2 = assistant2(messages, session_id)\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chain.invoke(messages)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mtraced_agent.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(messages, session_id, *args, **kwargs)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m propagate_session:\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m using_session(session_id):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     37\u001b[39m     result = fn(messages, session_id, *args, **kwargs)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 51\u001b[39m, in \u001b[36massistant\u001b[39m\u001b[34m(messages, session_id)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;129m@traced_agent\u001b[39m(name=\u001b[33m\"\u001b[39m\u001b[33mapp1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34massistant\u001b[39m(messages: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m], session_id: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m     50\u001b[39m     \u001b[38;5;66;03m# now you only have to do your business logic\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mchain\u001b[49m.invoke(messages)\n",
      "\u001b[31mNameError\u001b[39m: name 'chain' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from openinference.instrumentation import using_session\n",
    "from openinference.semconv.trace import SpanAttributes\n",
    "from opentelemetry import trace # wherever your context manager lives\n",
    "from functools import wraps\n",
    "\n",
    "\n",
    "def traced_agent(name: str, propagate_session: bool = True, tracer_name:str=\"lmorbits-trace\"):\n",
    "    \"\"\"\n",
    "    Decorator that wraps a function in a span named `name`,\n",
    "    automatically sets SESSION_ID, INPUT_VALUE, and OUTPUT_VALUE,\n",
    "    and (optionally) enters using_session(session_id) around the call.\n",
    "    \"\"\"\n",
    "    tracer = trace.get_tracer(tracer_name)\n",
    "    def decorator(fn):\n",
    "        @wraps(fn)\n",
    "        def wrapper(messages: list[dict], session_id: str, *args, **kwargs):\n",
    "            # start the OpenTelemetry span\n",
    "            with tracer.start_as_current_span(\n",
    "                name=name,\n",
    "                attributes={SpanAttributes.OPENINFERENCE_SPAN_KIND: \"agent\"}\n",
    "            ) as span:\n",
    "                # record session and input\n",
    "                trace_id = span.get_span_context().trace_id\n",
    "                span_id = span.get_span_context().span_id\n",
    "                trace_id_hex = format(trace_id, \"032x\")  # Converts to 32-character hex\n",
    "                trace_url = f\"{ph_instrumentor.project_url}/traces/{trace_id_hex}?selectedSpanNodeId={span_id}\"\n",
    "                \n",
    "                span.set_attribute(SpanAttributes.SESSION_ID, session_id)\n",
    "                last_msg = messages[-1].get(\"content\")\n",
    "                span.set_attribute(SpanAttributes.INPUT_VALUE, last_msg)\n",
    "\n",
    "                # optionally propagate session into sub‚Äêspans\n",
    "                if propagate_session:\n",
    "                    with using_session(session_id):\n",
    "                        result = fn(messages, session_id, *args, **kwargs)\n",
    "                else:\n",
    "                    result = fn(messages, session_id, *args, **kwargs)\n",
    "\n",
    "                # record the output\n",
    "                # assume returned object has .content\n",
    "                output = getattr(result, \"content\", result)\n",
    "                span.set_attribute(SpanAttributes.OUTPUT_VALUE, output)\n",
    "\n",
    "                return result,trace_url\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "@traced_agent(name=\"app1\")\n",
    "def assistant(messages: list[dict], session_id: str):\n",
    "    # now you only have to do your business logic\n",
    "    return chain.invoke(messages)\n",
    "\n",
    "@traced_agent(name=\"app2\")\n",
    "def assistant2(messages: list[dict], session_id: str):\n",
    "    # now you only have to do your business logic\n",
    "    return chain.invoke(messages)\n",
    "\n",
    "\n",
    "@traced_agent(name=\"app3\")\n",
    "def assistant3(messages: list[dict], session_id: str):\n",
    "    answer, trace_url = assistant(messages, session_id)\n",
    "    answer2, trace_url2 = assistant2(messages, session_id)\n",
    "    return chain.invoke(messages)\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": \"hi! im bob\"}\n",
    "]\n",
    "session_id = str(uuid.uuid4())\n",
    "\n",
    "\n",
    "answer, trace_url = assistant3(messages, session_id)\n",
    "\n",
    "print(trace_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "assistant3() missing 1 required positional argument: 'session_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43massistant3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: assistant3() missing 1 required positional argument: 'session_id'"
     ]
    }
   ],
   "source": [
    "assistant3(messages,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before calling sample_function\n",
      "After calling sample_function\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of a well-structured decorator with proper documentation and error handling\n",
    "def decorator_template(arg1, arg2=None):\n",
    "    def decorator(func):\n",
    "        from functools import wraps\n",
    "        @wraps(func)  # Preserves the metadata of the original function\n",
    "        def wrapper(*args, **kwargs):\n",
    "            try:\n",
    "                # Pre-processing logic here\n",
    "                print(f\"Before calling {func.__name__}\")\n",
    "                \n",
    "                # Call the original function\n",
    "                result = func(*args, **kwargs)\n",
    "                \n",
    "                # Post-processing logic here\n",
    "                print(f\"After calling {func.__name__}\")\n",
    "                \n",
    "                return result\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Error handling logic\n",
    "                print(f\"Error in {func.__name__}: {str(e)}\")\n",
    "                raise     \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Example usage:\n",
    "@decorator_template(\"example\", arg2=\"test\")\n",
    "def sample_function(x):\n",
    "    return x * 2\n",
    "\n",
    "sample_function(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": \"hi! im bob\"}\n",
    "]\n",
    "answer = chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'role': 'user', 'content': 'hi! im bob'}]\\n\\n```\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746628828.377568  568758 chttp2_transport.cc:1201] ipv6:%5B::1%5D:4317: Got goaway [11] err=UNAVAILABLE:GOAWAY received; Error code: 11; Debug Text: ping_timeout {created_time:\"2025-05-07T17:40:28.377561+03:00\", http2_error:11, grpc_status:14}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "OITracer.start_as_current_span() got an unexpected keyword argument 'metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tracer.start_as_current_span(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m,kind=SpanKind.CLIENT,) \u001b[38;5;28;01mas\u001b[39;00m main_span: \n\u001b[32m      2\u001b[39m     main_span.add_event(\u001b[33m\"\u001b[39m\u001b[33mtest event\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtracer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart_as_current_span\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapp1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapp1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m app1_span: \n\u001b[32m      4\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m model.astream(\u001b[33m\"\u001b[39m\u001b[33mjust say hello\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      5\u001b[39m             \u001b[38;5;28mprint\u001b[39m(chunk, end=\u001b[33m\"\u001b[39m\u001b[33m|\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-macos-aarch64-none/lib/python3.12/contextlib.py:301\u001b[39m, in \u001b[36mcontextmanager.<locals>.helper\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhelper\u001b[39m(*args, **kwds):\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_GeneratorContextManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-macos-aarch64-none/lib/python3.12/contextlib.py:105\u001b[39m, in \u001b[36m_GeneratorContextManagerBase.__init__\u001b[39m\u001b[34m(self, func, args, kwds)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, args, kwds):\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mself\u001b[39m.func, \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds = func, args, kwds\n\u001b[32m    107\u001b[39m     \u001b[38;5;66;03m# Issue 19330: ensure context manager instances have good docstrings\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: OITracer.start_as_current_span() got an unexpected keyword argument 'metadata'"
     ]
    }
   ],
   "source": [
    "with tracer.start_as_current_span(f\"test\",kind=SpanKind.CLIENT,) as main_span: \n",
    "    main_span.add_event(\"test event\")\n",
    "    with tracer.start_as_current_span(f\"app1\",metadata=dict(app=\"app1\")) as app1_span: \n",
    "        async for chunk in model.astream(\"just say hello\"):\n",
    "            print(chunk, end=\"|\", flush=True)\n",
    "    with tracer.start_as_current_span(f\"app2\",kind=SpanKind.INTERNAL) as app2_span: \n",
    "        async for chunk in model.astream(\"just say hello\"):\n",
    "            print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpanKind.INTERNAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-07 16:35:04.252\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1m['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/src/pyapp/observation'), 'status']\u001b[0m\n",
      "\u001b[32m2025-05-07 16:35:04.296\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m32\u001b[0m - \u001b[31m\u001b[1m\u001b[32mtask: [status] docker ps | grep lmorbits-obeservation | wc -l\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-05-07 16:35:04.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m       0\n",
      "\u001b[0m\n",
      "\u001b[32m2025-05-07 16:35:04.297\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1m['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/src/pyapp/observation'), 'start', 'PORT=6006', 'GRPC_PORT=4317']\u001b[0m\n",
      "\u001b[32m2025-05-07 16:35:06.206\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m32\u001b[0m - \u001b[31m\u001b[1m\u001b[32mtask: [start] docker pull arizephoenix/phoenix:latest\n",
      "\u001b[0m\u001b[32mtask: [start] docker run -d --name lmorbits-obeservation -p 6006:6006 -p 4317:4317 arizephoenix/phoenix:latest\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-05-07 16:35:06.207\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mlatest: Pulling from arizephoenix/phoenix\n",
      "Digest: sha256:3d1baca9d83f83e4034569325152b5b528088b4a5ad39fc8f91d6af8602f87f2\n",
      "Status: Image is up to date for arizephoenix/phoenix:latest\n",
      "docker.io/arizephoenix/phoenix:latest\n",
      "a405c552faa9947270ba1bda9a225e8e98acdd4377056d2729da64d3e9412619\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pyapp.observation.phoneix import PhoenixObservation\n",
    "\n",
    "phoenix = PhoenixObservation()\n",
    "answer = phoenix.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-07 16:01:17.790\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1m['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/src/pyapp/observation'), 'status']\u001b[0m\n",
      "\u001b[32m2025-05-07 16:01:17.839\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m32\u001b[0m - \u001b[31m\u001b[1m\u001b[32mtask: [status] docker ps | grep lmorbits-obeservation | wc -l\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-05-07 16:01:17.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m       0\n",
      "\u001b[0m\n",
      "\u001b[32m2025-05-07 16:01:17.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyapp.observation.phoneix\u001b[0m:\u001b[36mstop\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mPhoenix is not running\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "phoenix.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-07 16:34:55.461\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1m['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/src/pyapp/observation'), 'remove']\u001b[0m\n",
      "\u001b[32m2025-05-07 16:34:56.106\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m32\u001b[0m - \u001b[31m\u001b[1m\u001b[32mtask: [stop] docker stop lmorbits-obeservation\n",
      "\u001b[0m\u001b[32mtask: [remove] docker rm lmorbits-obeservation\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-05-07 16:34:56.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mlmorbits-obeservation\n",
      "lmorbits-obeservation\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "phoenix.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'args',\n",
       " 'check_returncode',\n",
       " 'returncode',\n",
       " 'stderr',\n",
       " 'stdout']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dir(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43manswer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "answer.stdout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
