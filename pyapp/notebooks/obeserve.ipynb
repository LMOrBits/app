{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# testing the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May 8, 2025 > 13:29:58 |  pyapp.observation.phoneix : 17 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Starting Phoenix observation \u001b[0m \n",
      "May 8, 2025 > 13:29:58 |  taskpy.main : 28 |  \u001b[34m\u001b[1m üêû DEBUG  |  ['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/src/pyapp/observation'), 'status'] \u001b[0m \n",
      "May 8, 2025 > 13:29:58 |  taskpy.main : 32 |  \u001b[31m\u001b[1m ‚ùå ERROR  |  \u001b[32mtask: [status] docker ps | grep lmorbits-obeservation | wc -l\n",
      "\u001b[0m \u001b[0m \n",
      "May 8, 2025 > 13:29:58 |  taskpy.main : 34 |  \u001b[1m ‚ÑπÔ∏è INFO  |         1\n",
      " \u001b[0m \n",
      "May 8, 2025 > 13:29:58 |  pyapp.observation.instance : 34 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Phoenix is already running \u001b[0m \n",
      "True\n",
      "May 8, 2025 > 13:29:59 |  pyapp.serve_integration.mlflow_llamacpp : 23 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Config path: /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/model_dir \u001b[0m \n",
      "May 8, 2025 > 13:29:59 |  pyapp.serve_integration.mlflow_llamacpp : 24 |  \u001b[1m ‚ÑπÔ∏è INFO  |  MLflow client: <mlflow.tracking.client.MlflowClient object at 0x117aefc50> \u001b[0m \n",
      "May 8, 2025 > 13:29:59 |  pyapp.serve_integration.mlflow_llamacpp : 26 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Experiments: [<Experiment: artifact_location='mlflow-artifacts:/0', creation_time=1745927919661, experiment_id='0', last_update_time=1745927919661, lifecycle_stage='active', name='Default', tags={}>] \u001b[0m \n",
      "May 8, 2025 > 13:29:59 |  serve.servers.llamacpp.serve : 54 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized LLaMA CPP server manager \u001b[0m \n",
      "May 8, 2025 > 13:29:59 |  serve.utils.model_config : 66 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized model configuration manager with config path: /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/model_dir \u001b[0m \n",
      "May 8, 2025 > 13:29:59 |  serve.utils.mlflow.config : 127 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized MLflow model configuration manager \u001b[0m \n",
      "May 8, 2025 > 13:29:59 |  serve.experiment_tracker.mlflow.mlflow_llamacpp.manager : 88 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized model manager successfully \u001b[0m \n",
      "May 8, 2025 > 13:29:59 |  pyapp.model_connection.lm.langchain.litellm : 17 |  \u001b[1m ‚ÑπÔ∏è INFO  |  connecing to mlflow ... \u001b[0m \n",
      "May 8, 2025 > 13:29:59 |  serve.servers.llamacpp.serve : 164 |  \u001b[31m\u001b[1m ‚ùå ERROR  |  Failed to get status for model rag_model: Model ID 'rag_model' not found \u001b[0m \n",
      "May 8, 2025 > 13:29:59 |  serve._cli.task : 29 |  \u001b[34m\u001b[1m üêû DEBUG  |  ['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/serve/src/serve/servers/llamacpp'), 'serve', 'MODEL_PATH=/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/models/rag_model/model_path/artifacts', 'PORT=8080', 'MODEL_NAME=model.gguf', 'MODEL_ID=rag_model'] \u001b[0m \n",
      "May 8, 2025 > 13:29:59 |  serve._cli.task : 33 |  \u001b[31m\u001b[1m ‚ùå ERROR  |  \u001b[32mtask: [serve] echo \"Starting LlamaCpp server and waiting for it to be ready...\"\n",
      "\u001b[0m\u001b[32mtask: [serve] container_id=$(docker run -d \\\n",
      "  --name lmorbits-llamacpp-rag_model \\\n",
      "  -p 8080:8080 \\\n",
      "  -v /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/models/rag_model/model_path/artifacts:/models \\\n",
      "  ghcr.io/ggerganov/llama.cpp:server \\\n",
      "  -m /models/model.gguf)\n",
      "\n",
      "while ! docker logs $container_id 2>&1 | grep -q \"GET /health 127.0.0.1 200\"; do\n",
      "  echo \"Loading model and starting server...\"\n",
      "  docker logs --tail 10 $container_id\n",
      "  sleep 1\n",
      "done\n",
      "echo \"LlamaCpp server is ready!\"\n",
      "\n",
      "\u001b[0mdocker: Error response from daemon: Conflict. The container name \"/lmorbits-llamacpp-rag_model\" is already in use by container \"1c417611e9797bbae3868a17c1a7ea12d0d9a3529ea090c98d679ee3f4cf5075\". You have to remove (or rename) that container to be able to reuse that name.\n",
      "See 'docker run --help'.\n",
      "\u001b[31mtask: Failed to run task \"serve\": exit status 125\n",
      "\u001b[0m \u001b[0m \n",
      "May 8, 2025 > 13:29:59 |  serve._cli.task : 35 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Starting LlamaCpp server and waiting for it to be ready...\n",
      " \u001b[0m \n",
      "May 8, 2025 > 13:29:59 |  serve.servers.llamacpp.serve : 140 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Started server for model ID: rag_model \u001b[0m \n",
      "May 8, 2025 > 13:29:59 |  serve.servers.llamacpp.serve : 91 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Added new server instance with model ID: rag_model \u001b[0m \n",
      "May 8, 2025 > 13:29:59 |  serve.experiment_tracker.mlflow.mlflow_llamacpp.manager : 198 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Started serving model rag_model on ports 8080 \u001b[0m \n"
     ]
    }
   ],
   "source": [
    "from pyapp.observation.phoneix import traced_agent,ph_instrumentor\n",
    "\n",
    "# -------------------model -------------------#\n",
    "from pyapp.model_connection.lm.langchain.litellm import get_model_mlflow_llamacpp, ModelConfig\n",
    "### --- config --- ###\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "here = Path(\"./\").resolve()\n",
    "print(Path(here / \"config.env\").exists())\n",
    "load_dotenv(here / \"config.env\")\n",
    "\n",
    "mlflow_port = os.getenv(\"MLFLOW_PORT\") or \"5001\"\n",
    "tracking_uri=f\"http://localhost:{mlflow_port}\"\n",
    "config_path=here / \"model_dir\"\n",
    "\n",
    "### --- model --- ###\n",
    "model_config = ModelConfig(model_name=\"rag_model\",port=8080,gguf_relative_path=\"model_path/artifacts/model.gguf\")\n",
    "model = get_model_mlflow_llamacpp(tracking_uri,config_path,model_config , stream=True , mock_response=\"Hello world\", temperature=0, max_tokens=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyapp.observation.phoneix import async_generator_traced_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1746700531.673991  428614 chttp2_transport.cc:1201] ipv6:%5B::1%5D:4317: Got goaway [11] err=UNAVAILABLE:GOAWAY received; Error code: 11; Debug Text: ping_timeout {grpc_status:14, http2_error:11, created_time:\"2025-05-08T13:35:31.673972+03:00\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "{'\n",
      "role\n",
      "':\n",
      " '\n",
      "user\n",
      "',\n",
      " '\n",
      "content\n",
      "':\n",
      " '\n",
      "hi\n",
      "!\n",
      " im\n",
      " bob\n",
      "'}\n",
      "]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "```\n",
      "\n",
      "http://localhost:6006/projects/UHJvamVjdDoy/traces/5ca06be7d46098e702522b670f896581?selectedSpanNodeId=U3Bhbjoy\n"
     ]
    }
   ],
   "source": [
    "# -------------------traced_agent -------------------#\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "@traced_agent(name=\"app1\")\n",
    "def assistant(messages: list[dict], session_id: str):\n",
    "    # now you only have to do your business logic\n",
    "   return chain.invoke(messages) \n",
    "\n",
    "@traced_agent(name=\"app2\")\n",
    "def assistant2(messages: list[dict], session_id: str):\n",
    "    # now you only have to do your business logic\n",
    "    return chain.invoke(messages)\n",
    "\n",
    "\n",
    "@async_generator_traced_agent(name=\"app3\")\n",
    "async def assistant3(messages: list[dict], session_id: str):\n",
    "    answer, trace_url =  assistant(messages, session_id)\n",
    "    answer2, trace_url2 = assistant2(messages, session_id)\n",
    "    answer = \"\"\n",
    "    async for chunks in chain.astream(messages):\n",
    "        answer += chunks.content\n",
    "        yield chunks\n",
    "\n",
    "\n",
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": \"hi! im bob\"}\n",
    "]\n",
    "import uuid\n",
    "session_id = str(uuid.uuid4())\n",
    "# answer, trace_url = await assistant3(messages, session_id)\n",
    "# print(trace_url)\n",
    "linked_url = None\n",
    "async for chunk in assistant3(messages, session_id):\n",
    "    print(chunk[0].content)\n",
    "    linked_url = chunk[1]\n",
    "print(linked_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
