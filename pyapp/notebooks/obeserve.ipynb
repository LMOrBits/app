{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "May 7, 2025 > 17:03:09 |  pyapp.serve_integration.mlflow_llamacpp : 23 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Config path: /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/model_dir \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  pyapp.serve_integration.mlflow_llamacpp : 24 |  \u001b[1m ‚ÑπÔ∏è INFO  |  MLflow client: <mlflow.tracking.client.MlflowClient object at 0x10925efc0> \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  pyapp.serve_integration.mlflow_llamacpp : 26 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Experiments: [<Experiment: artifact_location='mlflow-artifacts:/0', creation_time=1745927919661, experiment_id='0', last_update_time=1745927919661, lifecycle_stage='active', name='Default', tags={}>] \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  serve.servers.llamacpp.serve : 54 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized LLaMA CPP server manager \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  serve.utils.model_config : 66 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized model configuration manager with config path: /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/model_dir \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  serve.utils.mlflow.config : 127 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized MLflow model configuration manager \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  serve.experiment_tracker.mlflow.mlflow_llamacpp.manager : 88 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized model manager successfully \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  pyapp.model_connection.lm.langchain.litellm : 17 |  \u001b[1m ‚ÑπÔ∏è INFO  |  connecing to mlflow ... \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  serve.servers.llamacpp.serve : 164 |  \u001b[31m\u001b[1m ‚ùå ERROR  |  Failed to get status for model rag_model: Model ID 'rag_model' not found \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  serve._cli.task : 29 |  \u001b[34m\u001b[1m üêû DEBUG  |  ['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/serve/src/serve/servers/llamacpp'), 'serve', 'MODEL_PATH=/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/models/rag_model/model_path/artifacts', 'PORT=8080', 'MODEL_NAME=model.gguf', 'MODEL_ID=rag_model'] \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  serve._cli.task : 33 |  \u001b[31m\u001b[1m ‚ùå ERROR  |  \u001b[32mtask: [serve] echo \"Starting LlamaCpp server and waiting for it to be ready...\"\n",
      "\u001b[0m\u001b[32mtask: [serve] container_id=$(docker run -d \\\n",
      "  --name lmorbits-llamacpp-rag_model \\\n",
      "  -p 8080:8080 \\\n",
      "  -v /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/models/rag_model/model_path/artifacts:/models \\\n",
      "  ghcr.io/ggerganov/llama.cpp:server \\\n",
      "  -m /models/model.gguf)\n",
      "\n",
      "while ! docker logs $container_id 2>&1 | grep -q \"GET /health 127.0.0.1 200\"; do\n",
      "  echo \"Loading model and starting server...\"\n",
      "  docker logs --tail 10 $container_id\n",
      "  sleep 1\n",
      "done\n",
      "echo \"LlamaCpp server is ready!\"\n",
      "\n",
      "\u001b[0mdocker: Error response from daemon: Conflict. The container name \"/lmorbits-llamacpp-rag_model\" is already in use by container \"c7dc2b3387c4e61802e943b8b33baa7d1a678c8b6679041c7560754d869a9ed6\". You have to remove (or rename) that container to be able to reuse that name.\n",
      "See 'docker run --help'.\n",
      "\u001b[31mtask: Failed to run task \"serve\": exit status 125\n",
      "\u001b[0m \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  serve._cli.task : 35 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Starting LlamaCpp server and waiting for it to be ready...\n",
      " \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  serve.servers.llamacpp.serve : 140 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Started server for model ID: rag_model \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  serve.servers.llamacpp.serve : 91 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Added new server instance with model ID: rag_model \u001b[0m \n",
      "May 7, 2025 > 17:03:09 |  serve.experiment_tracker.mlflow.mlflow_llamacpp.manager : 198 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Started serving model rag_model on ports 8080 \u001b[0m \n"
     ]
    }
   ],
   "source": [
    "from pyapp.model_connection.lm.langchain.litellm import get_model_mlflow_llamacpp, ModelConfig\n",
    "### --- config --- ###\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "here = Path(\"./\").resolve()\n",
    "print(Path(here / \"config.env\").exists())\n",
    "load_dotenv(here / \"config.env\")\n",
    "\n",
    "mlflow_port = os.getenv(\"MLFLOW_PORT\") or \"5001\"\n",
    "tracking_uri=f\"http://localhost:{mlflow_port}\"\n",
    "config_path=here / \"model_dir\"\n",
    "\n",
    "### --- model --- ###\n",
    "model_config = ModelConfig(model_name=\"rag_model\",port=8080,gguf_relative_path=\"model_path/artifacts/model.gguf\")\n",
    "model = get_model_mlflow_llamacpp(tracking_uri,config_path,model_config , stream=True , mock_response=\"Hello world\", temperature=0, max_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to instrument while already instrumented\n",
      "Overriding of current TracerProvider is not allowed\n"
     ]
    }
   ],
   "source": [
    "from opentelemetry.trace import SpanKind\n",
    "from pyapp.observation.phoneix import PhonexLangChainInstrumentor\n",
    "observation = PhonexLangChainInstrumentor(\"lmorbits-phoenix\",\"app2\")\n",
    "tracer = observation.get_tracer(\"__name__\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to instrument while already instrumented\n"
     ]
    }
   ],
   "source": [
    "from loguru import logger\n",
    "from taskpy import TaskCLI\n",
    "from pathlib import Path\n",
    "tracer = None\n",
    "\n",
    "class PhoenixObservation:\n",
    "  @property\n",
    "  def port(self):\n",
    "    return 6006\n",
    "\n",
    "  @property\n",
    "  def grpc_port(self):\n",
    "    return 4317\n",
    "\n",
    "  def __init__(self):\n",
    "    self.task = TaskCLI(Path(__file__).parent )\n",
    "  \n",
    "  \n",
    "  def is_running(self):\n",
    "      try:\n",
    "        answer = self.task.run(\"status\")\n",
    "        return int(answer.stdout.strip())!= 0\n",
    "      except Exception as e:\n",
    "        logger.error(f\"Error in phoenix observation: {e}\")\n",
    "        return False \n",
    "\n",
    "  def start(self):\n",
    "    try:\n",
    "      status = self.is_running()\n",
    "      if status:\n",
    "        logger.info(\"Phoenix is already running\")\n",
    "      else: \n",
    "        self.task.run(\"start\", port=self.port, grpc_port=self.grpc_port)\n",
    "    except Exception as e:\n",
    "      logger.error(f\"Error in phoenix observation: {e}\")\n",
    "\n",
    "  def stop(self):\n",
    "    try:\n",
    "      status = self.is_running()\n",
    "      if not status:\n",
    "        logger.info(\"Phoenix is not running\")\n",
    "      else:\n",
    "        self.task.run(\"stop\")\n",
    "    except Exception as e:\n",
    "      logger.error(f\"Error in phoenix observation: {e}\")\n",
    "  \n",
    "  def remove(self):\n",
    "    try:\n",
    "      self.task.run(\"remove\")\n",
    "    except Exception as e:\n",
    "      logger.error(f\"Error in phoenix observation: {e}\")\n",
    "\n",
    "\n",
    "class PhonexLangChainInstrumentor:\n",
    "  tracer_provider = None\n",
    "\n",
    "  def __init__(self, project_name: str= \"lmorbits-phoenix\" , app_name: str= \"app1\"):\n",
    "    try:\n",
    "      self.app_name = app_name\n",
    "      self.project_name = project_name\n",
    "      from opentelemetry import trace\n",
    "      from openinference.instrumentation.langchain import LangChainInstrumentor\n",
    "      from phoenix.otel import register\n",
    "\n",
    "      self.tracer_provider = register(\n",
    "        project_name=self.project_name,\n",
    "        set_global_tracer_provider=False,\n",
    "        verbose=False\n",
    "      ) \n",
    "      LangChainInstrumentor().instrument(tracer_provider=self.tracer_provider)\n",
    "      trace.set_tracer_provider(self.tracer_provider)\n",
    "      \n",
    "\n",
    "    except Exception as e:\n",
    "      logger.error(f\"Error in phoenix observation: {e}\")\n",
    "  \n",
    "  def get_tracer(self , module_name:str):\n",
    "    from opentelemetry import trace\n",
    "    return trace.get_tracer(f\"{self.app_name}.{module_name}\")\n",
    "    \n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation import using_metadata\n",
    "metadata = {\n",
    "    \"key-1\": \"value_1\",\n",
    "    \"key-2\": \"value_2\",\n",
    "}\n",
    "\n",
    "@using_metadata(metadata)\n",
    "def call_fn(*args, **kwargs):\n",
    "    print(\"call_fn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openinference.instrumentation import TraceConfig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.arize.com/phoenix/tracing/how-to-tracing/setup-tracing/setup-sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='\\n' additional_kwargs={} response_metadata={'model_name': 'openai/custom'} id='run--e538bff6-985f-454a-ba3a-deeace29e737' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}|content='```' additional_kwargs={} response_metadata={} id='run--e538bff6-985f-454a-ba3a-deeace29e737' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}|content='\\n' additional_kwargs={} response_metadata={} id='run--e538bff6-985f-454a-ba3a-deeace29e737' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}|content='\\n' additional_kwargs={} response_metadata={} id='run--e538bff6-985f-454a-ba3a-deeace29e737' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}|content='I' additional_kwargs={} response_metadata={} id='run--e538bff6-985f-454a-ba3a-deeace29e737' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}|content=' have' additional_kwargs={} response_metadata={} id='run--e538bff6-985f-454a-ba3a-deeace29e737' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}|content=' tried' additional_kwargs={} response_metadata={} id='run--e538bff6-985f-454a-ba3a-deeace29e737' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}|content=' to' additional_kwargs={} response_metadata={} id='run--e538bff6-985f-454a-ba3a-deeace29e737' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}|content=' use' additional_kwargs={} response_metadata={} id='run--e538bff6-985f-454a-ba3a-deeace29e737' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}|content=' `' additional_kwargs={} response_metadata={} id='run--e538bff6-985f-454a-ba3a-deeace29e737' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}|content='@' additional_kwargs={} response_metadata={} id='run--e538bff6-985f-454a-ba3a-deeace29e737' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}|content='{' additional_kwargs={} response_metadata={} id='run--e538bff6-985f-454a-ba3a-deeace29e737' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}|content='@' additional_kwargs={} response_metadata={} id='run--e538bff6-985f-454a-ba3a-deeace29e737' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}|content='{' additional_kwargs={} response_metadata={} id='run--e538bff6-985f-454a-ba3a-deeace29e737' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}|content='@' additional_kwargs={} response_metadata={} id='run--e538bff6-985f-454a-ba3a-deeace29e737' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}|content='{' additional_kwargs={} response_metadata={} id='run--e538bff6-985f-454a-ba3a-deeace29e737' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}|content='@' additional_kwargs={} response_metadata={} id='run--e538bff6-985f-454a-ba3a-deeace29e737' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}|content='{' additional_kwargs={} response_metadata={} id='run--e538bff6-985f-454a-ba3a-deeace29e737' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}|content='@' additional_kwargs={} response_metadata={} id='run--e538bff6-985f-454a-ba3a-deeace29e737' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}|content='{' additional_kwargs={} response_metadata={} id='run--e538bff6-985f-454a-ba3a-deeace29e737' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}|content='' additional_kwargs={} response_metadata={} id='run--e538bff6-985f-454a-ba3a-deeace29e737' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0}|"
     ]
    }
   ],
   "source": [
    "\n",
    "from openinference.instrumentation import using_attributes, using_user , using_metadata, using_session\n",
    "from openinference.semconv.trace import SpanAttributes\n",
    "import uuid\n",
    "from opentelemetry import trace\n",
    "\n",
    "tracer = trace.get_tracer(\"1\")\n",
    "\n",
    "\n",
    "async def app1(message:str, session_id:str):\n",
    "\n",
    "    with tracer.start_as_current_span(f\"app1\",kind=SpanKind.INTERNAL) as app1_span:\n",
    "        app1_span.set_attribute(SpanAttributes.SESSION_ID, session_id)\n",
    "        app1_span.set_attribute(SpanAttributes.INPUT_VALUE, message)\n",
    "        with using_session(session_id):\n",
    "            answer = \"\"\n",
    "            async for chunk in model.astream(message):\n",
    "                answer += chunk.content\n",
    "                print(chunk, end=\"|\", flush=True)\n",
    "    \n",
    "        app1_span.set_attribute(SpanAttributes.OUTPUT_VALUE, answer)\n",
    "    return answer\n",
    "\n",
    "# tracer2 = trace.get_tracer(\"2\")\n",
    "# @tracer2.start_as_current_span(f\"app2\",kind=SpanKind.INTERNAL, attributes={SpanAttributes.OPENINFERENCE_SPAN_KIND: \"agent\"})\n",
    "# async def app2(message:str, session_id:str):\n",
    "#     current_span = trace.get_current_span()\n",
    "#     current_span.set_attribute(SpanAttributes.SESSION_ID, session_id)\n",
    "#     current_span.set_attribute(SpanAttributes.INPUT_VALUE, message)\n",
    "#     with using_session(session_id):\n",
    "#         answer = \"\"\n",
    "#         async for chunk in model.astream(message):\n",
    "#             answer += chunk\n",
    "#             print(chunk, end=\"|\", flush=True)\n",
    "    \n",
    "#     current_span.set_attribute(SpanAttributes.OUTPUT_VALUE, answer)\n",
    "#     return answer\n",
    "\n",
    "# prompt_template = \"Please describe the weather forecast for {city} on {date}\"\n",
    "# prompt_template_variables = {\"city\": \"Johannesburg\", \"date\":\"July 11\"}\n",
    "# prompt_template_version = \"v1.0\"\n",
    "# with using_attributes(\n",
    "#     session_id=\"my-session-id\",\n",
    "#     user_id=\"my-user-id\",\n",
    "#     metadata=metadata,\n",
    "#     tags=tags,\n",
    "#     prompt_template=prompt_template,\n",
    "#     prompt_template_version=prompt_template_version,\n",
    "#     prompt_template_variables=prompt_template_variables,\n",
    "# ):\n",
    "    \n",
    "\n",
    "with tracer.start_as_current_span(f\"test\",kind=SpanKind.CLIENT,) as main_span:\n",
    "    session_id = str(uuid.uuid4())\n",
    "    await app1(\"hello\", session_id)\n",
    "# app2(\"i\", session_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "import openai\n",
    "from openinference.instrumentation import using_session\n",
    "from openinference.semconv.trace import SpanAttributes\n",
    "from opentelemetry import trace\n",
    "\n",
    "session_id = str(uuid.uuid4())\n",
    "\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant.\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "@tracer.start_as_current_span(name=\"app1\", attributes={SpanAttributes.OPENINFERENCE_SPAN_KIND: \"agent\"})\n",
    "def assistant(\n",
    "  messages: list[dict],\n",
    "  session_id: str = str,\n",
    "):\n",
    "  current_span = trace.get_current_span()\n",
    "  current_span.set_attribute(SpanAttributes.SESSION_ID, session_id)\n",
    "  current_span.set_attribute(SpanAttributes.INPUT_VALUE, messages[-1].get('content'))\n",
    "\n",
    "  # Propagate the session_id down to spans crated by the OpenAI instrumentation\n",
    "  # This is not strictly necessary, but it helps to correlate the spans to the same session\n",
    "  with using_session(session_id):\n",
    "     response = chain.invoke(messages)\n",
    "     \n",
    "\n",
    "  current_span.set_attribute(SpanAttributes.OUTPUT_VALUE, response.content)\n",
    "  return response\n",
    "\n",
    "@tracer.start_as_current_span(name=\"app2\", attributes={SpanAttributes.OPENINFERENCE_SPAN_KIND: \"agent\"})\n",
    "def assistant2(\n",
    "  messages: list[dict],\n",
    "  session_id: str = str,\n",
    "):\n",
    "  current_span = trace.get_current_span()\n",
    "  current_span.set_attribute(SpanAttributes.SESSION_ID, session_id)\n",
    "  current_span.set_attribute(SpanAttributes.INPUT_VALUE, messages[-1].get('content'))\n",
    "\n",
    "  # Propagate the session_id down to spans crated by the OpenAI instrumentation\n",
    "  # This is not strictly necessary, but it helps to correlate the spans to the same session\n",
    "  with using_session(session_id):\n",
    "     response = chain.invoke(messages)\n",
    "     \n",
    "\n",
    "  current_span.set_attribute(SpanAttributes.OUTPUT_VALUE, response.content)\n",
    "  return response\n",
    "\n",
    "with tracer.start_as_current_span(f\"test\",kind=SpanKind.CLIENT,) as main_span:\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"hi! im bob\"}\n",
    "    ]\n",
    "    response = assistant(\n",
    "      messages,\n",
    "      session_id=session_id,\n",
    "    )\n",
    "    response2 = assistant2(\n",
    "      messages,\n",
    "      session_id=session_id,\n",
    "    )\n",
    "# messages = messages + [\n",
    "#   response,\n",
    "#   {\"role\": \"user\", \"content\": \"what's my name?\"}\n",
    "# ]\n",
    "# response = assistant(\n",
    "#   messages,\n",
    "#   session_id=session_id,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "  {\"role\": \"user\", \"content\": \"hi! im bob\"}\n",
    "]\n",
    "answer = chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[{'role': 'user', 'content': 'hi! im bob'}]\\n\\n```\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1746628828.377568  568758 chttp2_transport.cc:1201] ipv6:%5B::1%5D:4317: Got goaway [11] err=UNAVAILABLE:GOAWAY received; Error code: 11; Debug Text: ping_timeout {created_time:\"2025-05-07T17:40:28.377561+03:00\", http2_error:11, grpc_status:14}\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "OITracer.start_as_current_span() got an unexpected keyword argument 'metadata'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m tracer.start_as_current_span(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m,kind=SpanKind.CLIENT,) \u001b[38;5;28;01mas\u001b[39;00m main_span: \n\u001b[32m      2\u001b[39m     main_span.add_event(\u001b[33m\"\u001b[39m\u001b[33mtest event\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtracer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart_as_current_span\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapp1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mapp\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapp1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m app1_span: \n\u001b[32m      4\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m model.astream(\u001b[33m\"\u001b[39m\u001b[33mjust say hello\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m      5\u001b[39m             \u001b[38;5;28mprint\u001b[39m(chunk, end=\u001b[33m\"\u001b[39m\u001b[33m|\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-macos-aarch64-none/lib/python3.12/contextlib.py:301\u001b[39m, in \u001b[36mcontextmanager.<locals>.helper\u001b[39m\u001b[34m(*args, **kwds)\u001b[39m\n\u001b[32m    299\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhelper\u001b[39m(*args, **kwds):\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_GeneratorContextManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.7-macos-aarch64-none/lib/python3.12/contextlib.py:105\u001b[39m, in \u001b[36m_GeneratorContextManagerBase.__init__\u001b[39m\u001b[34m(self, func, args, kwds)\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, args, kwds):\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28mself\u001b[39m.gen = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     \u001b[38;5;28mself\u001b[39m.func, \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds = func, args, kwds\n\u001b[32m    107\u001b[39m     \u001b[38;5;66;03m# Issue 19330: ensure context manager instances have good docstrings\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: OITracer.start_as_current_span() got an unexpected keyword argument 'metadata'"
     ]
    }
   ],
   "source": [
    "with tracer.start_as_current_span(f\"test\",kind=SpanKind.CLIENT,) as main_span: \n",
    "    main_span.add_event(\"test event\")\n",
    "    with tracer.start_as_current_span(f\"app1\",metadata=dict(app=\"app1\")) as app1_span: \n",
    "        async for chunk in model.astream(\"just say hello\"):\n",
    "            print(chunk, end=\"|\", flush=True)\n",
    "    with tracer.start_as_current_span(f\"app2\",kind=SpanKind.INTERNAL) as app2_span: \n",
    "        async for chunk in model.astream(\"just say hello\"):\n",
    "            print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SpanKind.INTERNAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-07 16:35:04.252\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1m['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/src/pyapp/observation'), 'status']\u001b[0m\n",
      "\u001b[32m2025-05-07 16:35:04.296\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m32\u001b[0m - \u001b[31m\u001b[1m\u001b[32mtask: [status] docker ps | grep lmorbits-obeservation | wc -l\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-05-07 16:35:04.296\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m       0\n",
      "\u001b[0m\n",
      "\u001b[32m2025-05-07 16:35:04.297\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1m['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/src/pyapp/observation'), 'start', 'PORT=6006', 'GRPC_PORT=4317']\u001b[0m\n",
      "\u001b[32m2025-05-07 16:35:06.206\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m32\u001b[0m - \u001b[31m\u001b[1m\u001b[32mtask: [start] docker pull arizephoenix/phoenix:latest\n",
      "\u001b[0m\u001b[32mtask: [start] docker run -d --name lmorbits-obeservation -p 6006:6006 -p 4317:4317 arizephoenix/phoenix:latest\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-05-07 16:35:06.207\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mlatest: Pulling from arizephoenix/phoenix\n",
      "Digest: sha256:3d1baca9d83f83e4034569325152b5b528088b4a5ad39fc8f91d6af8602f87f2\n",
      "Status: Image is up to date for arizephoenix/phoenix:latest\n",
      "docker.io/arizephoenix/phoenix:latest\n",
      "a405c552faa9947270ba1bda9a225e8e98acdd4377056d2729da64d3e9412619\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pyapp.observation.phoneix import PhoenixObservation\n",
    "\n",
    "phoenix = PhoenixObservation()\n",
    "answer = phoenix.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-07 16:01:17.790\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1m['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/src/pyapp/observation'), 'status']\u001b[0m\n",
      "\u001b[32m2025-05-07 16:01:17.839\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m32\u001b[0m - \u001b[31m\u001b[1m\u001b[32mtask: [status] docker ps | grep lmorbits-obeservation | wc -l\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-05-07 16:01:17.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1m       0\n",
      "\u001b[0m\n",
      "\u001b[32m2025-05-07 16:01:17.839\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mpyapp.observation.phoneix\u001b[0m:\u001b[36mstop\u001b[0m:\u001b[36m32\u001b[0m - \u001b[1mPhoenix is not running\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "phoenix.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-05-07 16:34:55.461\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m28\u001b[0m - \u001b[34m\u001b[1m['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/src/pyapp/observation'), 'remove']\u001b[0m\n",
      "\u001b[32m2025-05-07 16:34:56.106\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m32\u001b[0m - \u001b[31m\u001b[1m\u001b[32mtask: [stop] docker stop lmorbits-obeservation\n",
      "\u001b[0m\u001b[32mtask: [remove] docker rm lmorbits-obeservation\n",
      "\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-05-07 16:34:56.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mtaskpy.main\u001b[0m:\u001b[36mrun\u001b[0m:\u001b[36m34\u001b[0m - \u001b[1mlmorbits-obeservation\n",
      "lmorbits-obeservation\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "phoenix.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__class_getitem__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'args',\n",
       " 'check_returncode',\n",
       " 'returncode',\n",
       " 'stderr',\n",
       " 'stdout']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dir(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43manswer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstdout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "answer.stdout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
