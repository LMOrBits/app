{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b78c4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "May 7, 2025 > 09:42:48 |  pyapp.serve_integration.mlflow_llamacpp : 24 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Config path: /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/model_dir \u001b[0m \n",
      "May 7, 2025 > 09:42:48 |  pyapp.serve_integration.mlflow_llamacpp : 25 |  \u001b[1m ‚ÑπÔ∏è INFO  |  MLflow client: <mlflow.tracking.client.MlflowClient object at 0x12473c920> \u001b[0m \n",
      "May 7, 2025 > 09:42:48 |  pyapp.serve_integration.mlflow_llamacpp : 27 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Experiments: [<Experiment: artifact_location='mlflow-artifacts:/0', creation_time=1745927919661, experiment_id='0', last_update_time=1745927919661, lifecycle_stage='active', name='Default', tags={}>] \u001b[0m \n",
      "May 7, 2025 > 09:42:48 |  serve.utils.model_config : 214 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Configuration saved successfully \u001b[0m \n",
      "May 7, 2025 > 09:42:48 |  serve.utils.model_config : 101 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Created empty configuration file: /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/model_dir/config.json \u001b[0m \n",
      "May 7, 2025 > 09:42:48 |  serve.utils.model_config : 66 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized model configuration manager with config path: /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/model_dir/config.json \u001b[0m \n",
      "May 7, 2025 > 09:42:48 |  serve.utils.mlflow.config : 127 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized MLflow model configuration manager \u001b[0m \n",
      "May 7, 2025 > 09:42:48 |  serve.servers.llamacpp.serve : 54 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized LLaMA CPP server manager \u001b[0m \n",
      "May 7, 2025 > 09:42:48 |  serve.utils.model_config : 66 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized model configuration manager with config path: /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/model_dir/config.json \u001b[0m \n",
      "May 7, 2025 > 09:42:48 |  serve.utils.mlflow.config : 127 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized MLflow model configuration manager \u001b[0m \n",
      "May 7, 2025 > 09:42:48 |  serve.experiment_tracker.mlflow.mlflow_llamacpp.manager : 88 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized model manager successfully \u001b[0m \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "here = Path(\"./\").resolve()\n",
    "print(Path(here / \"config.env\").exists())\n",
    "load_dotenv(here / \"config.env\")\n",
    "\n",
    "mlflow_port = os.getenv(\"MLFLOW_PORT\")\n",
    "minio_access_key = os.getenv(\"MINIO_ACCESS_KEY\")\n",
    "minio_secret_key = os.getenv(\"MINIO_SECRET_ACCESS_KEY\")\n",
    "\n",
    "(here/\"model_dir\").mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "from pyapp.serve_integration.mlflow_llamacpp import get_model_manager\n",
    "model_manager = get_model_manager(\n",
    "tracking_uri=f\"http://localhost:{mlflow_port}\",\n",
    "config_path=here / \"model_dir\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20900794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May 7, 2025 > 09:40:10 |  serve.utils.model_config : 159 |  \u001b[33m\u001b[1m ‚ö†Ô∏è WARNING  |  No configuration found for model: rag_model \u001b[0m \n",
      "May 7, 2025 > 09:40:10 |  serve.utils.mlflow.config : 237 |  \u001b[1m ‚ÑπÔ∏è INFO  |  No local version found for rag_model \u001b[0m \n",
      "May 7, 2025 > 09:40:10 |  serve.utils.mlflow.config : 181 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Model rag_model is already up to date \u001b[0m \n",
      "May 7, 2025 > 09:40:10 |  serve.utils.model_config : 159 |  \u001b[33m\u001b[1m ‚ö†Ô∏è WARNING  |  No configuration found for model: rag_model \u001b[0m \n",
      "May 7, 2025 > 09:40:10 |  serve.utils.mlflow.config : 460 |  \u001b[31m\u001b[1m ‚ùå ERROR  |  Error getting path for rag_model: No path found for model rag_model \u001b[0m \n",
      "May 7, 2025 > 09:40:10 |  serve.utils.mlflow.config : 189 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Found model version: 3 for rag_model \u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:01<00:00,  4.41it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May 7, 2025 > 09:40:11 |  serve.utils.mlflow.config : 151 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Downloaded artifacts to /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/d/models/rag_model \u001b[0m \n",
      "May 7, 2025 > 09:40:11 |  serve.utils.model_config : 159 |  \u001b[33m\u001b[1m ‚ö†Ô∏è WARNING  |  No configuration found for model: rag_model \u001b[0m \n",
      "May 7, 2025 > 09:40:11 |  serve.utils.model_config : 124 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Created model directory: /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/d/rag_model \u001b[0m \n",
      "May 7, 2025 > 09:40:11 |  serve.utils.model_config : 214 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Configuration saved successfully \u001b[0m \n",
      "May 7, 2025 > 09:40:11 |  serve.utils.model_config : 235 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Updated configuration for model: rag_model \u001b[0m \n",
      "May 7, 2025 > 09:40:11 |  serve.utils.model_config : 271 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Updated model info for rag_model with run_id ddce9533e62948278bf6c9ea0a65fcc9 \u001b[0m \n",
      "May 7, 2025 > 09:40:11 |  serve.experiment_tracker.mlflow.mlflow_llamacpp.manager : 127 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Successfully added model rag_model \u001b[0m \n",
      "May 7, 2025 > 09:40:11 |  serve._cli.task : 29 |  \u001b[34m\u001b[1m üêû DEBUG  |  ['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/.venv/lib/python3.12/site-packages/serve/servers/llamacpp'), 'serve', 'MODEL_PATH=/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/d/models/rag_model/model_path/artifacts', 'SERVER_PORTS=8000', 'UI_PORT=8080', 'MODEL_NAME=model.gguf', 'MODEL_ID=rag_model'] \u001b[0m \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May 7, 2025 > 09:40:42 |  serve._cli.task : 33 |  \u001b[31m\u001b[1m ‚ùå ERROR  |  \u001b[32mtask: [serve] echo \"Starting LlamaCpp server and waiting for it to be ready...\"\n",
      "\u001b[0m\u001b[32mtask: [serve] container_id=$(docker run -d \\\n",
      "  --name lmorbits-llamacpp-rag_model \\\n",
      "  -p 8000:8000 \\\n",
      "  -p 8080:8080 \\\n",
      "  -v /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/d/models/rag_model/model_path/artifacts:/models \\\n",
      "  ghcr.io/ggerganov/llama.cpp:server \\\n",
      "  -m /models/model.gguf)\n",
      "\n",
      "while ! docker logs $container_id 2>&1 | grep -q \"GET /health 127.0.0.1 200\"; do\n",
      "  echo \"Loading model and starting server...\"\n",
      "  docker logs --tail 10 $container_id\n",
      "  sleep 1\n",
      "done\n",
      "echo \"LlamaCpp server is ready!\"\n",
      "\n",
      "\u001b[0m<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      " \u001b[0m \n",
      "May 7, 2025 > 09:40:42 |  serve._cli.task : 35 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Starting LlamaCpp server and waiting for it to be ready...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "LlamaCpp server is ready!\n",
      " \u001b[0m \n",
      "May 7, 2025 > 09:40:42 |  serve.servers.llamacpp.serve : 145 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Started server for model ID: rag_model \u001b[0m \n",
      "May 7, 2025 > 09:40:42 |  serve.servers.llamacpp.serve : 94 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Added new server instance with model ID: rag_model \u001b[0m \n",
      "May 7, 2025 > 09:40:42 |  serve.experiment_tracker.mlflow.mlflow_llamacpp.manager : 176 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Started serving model rag_model on ports 8000/8080 \u001b[0m \n"
     ]
    }
   ],
   "source": [
    "model_manager.add_model(\"rag_model\", alias=\"champion\", artifact_path=\"model_path\")\n",
    "model_manager.add_serve(\"rag_model\", server_port=8000, ui_port=8080, gguf_relative_path=\"model_path/artifacts/model.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad90649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
