{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b78c4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "May 7, 2025 > 12:37:30 |  pyapp.serve_integration.mlflow_llamacpp : 24 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Config path: /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/model_dir \u001b[0m \n",
      "May 7, 2025 > 12:37:30 |  pyapp.serve_integration.mlflow_llamacpp : 25 |  \u001b[1m ‚ÑπÔ∏è INFO  |  MLflow client: <mlflow.tracking.client.MlflowClient object at 0x113083f20> \u001b[0m \n",
      "May 7, 2025 > 12:37:30 |  pyapp.serve_integration.mlflow_llamacpp : 27 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Experiments: [<Experiment: artifact_location='mlflow-artifacts:/0', creation_time=1745927919661, experiment_id='0', last_update_time=1745927919661, lifecycle_stage='active', name='Default', tags={}>] \u001b[0m \n",
      "May 7, 2025 > 12:37:30 |  serve.servers.llamacpp.serve : 54 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized LLaMA CPP server manager \u001b[0m \n",
      "May 7, 2025 > 12:37:30 |  serve.utils.model_config : 66 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized model configuration manager with config path: /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/model_dir/config.json \u001b[0m \n",
      "May 7, 2025 > 12:37:30 |  serve.utils.mlflow.config : 127 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized MLflow model configuration manager \u001b[0m \n",
      "May 7, 2025 > 12:37:30 |  serve.experiment_tracker.mlflow.mlflow_llamacpp.manager : 88 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Initialized model manager successfully \u001b[0m \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import os \n",
    "here = Path(\"./\").resolve()\n",
    "print(Path(here / \"config.env\").exists())\n",
    "load_dotenv(here / \"config.env\")\n",
    "\n",
    "mlflow_port = os.getenv(\"MLFLOW_PORT\") or \"5001\"\n",
    "minio_access_key = os.getenv(\"MINIO_ACCESS_KEY\")\n",
    "minio_secret_key = os.getenv(\"MINIO_SECRET_ACCESS_KEY\")\n",
    "\n",
    "(here/\"model_dir\").mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "from pyapp.serve_integration.mlflow_llamacpp import get_model_manager\n",
    "model_manager = get_model_manager(\n",
    "tracking_uri=f\"http://localhost:{mlflow_port}\",\n",
    "config_path=here / \"model_dir\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15366ffe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20900794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May 7, 2025 > 12:21:58 |  serve.servers.llamacpp.serve : 169 |  \u001b[31m\u001b[1m ‚ùå ERROR  |  Failed to get status for model rag_model: Model ID 'rag_model' not found \u001b[0m \n",
      "May 7, 2025 > 12:21:58 |  serve._cli.task : 29 |  \u001b[34m\u001b[1m üêû DEBUG  |  ['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/serve/src/serve/servers/llamacpp'), 'serve', 'MODEL_PATH=/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/model_dir/models/rag_model/model_path/artifacts', 'SERVER_PORTS=8000', 'UI_PORT=8080', 'MODEL_NAME=model.gguf', 'MODEL_ID=rag_model'] \u001b[0m \n",
      "May 7, 2025 > 12:22:29 |  serve._cli.task : 33 |  \u001b[31m\u001b[1m ‚ùå ERROR  |  \u001b[32mtask: [serve] echo \"Starting LlamaCpp server and waiting for it to be ready...\"\n",
      "\u001b[0m\u001b[32mtask: [serve] container_id=$(docker run -d \\\n",
      "  --name lmorbits-llamacpp-rag_model \\\n",
      "  -p 8000:8000 \\\n",
      "  -p 8080:8080 \\\n",
      "  -v /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/model_dir/models/rag_model/model_path/artifacts:/models \\\n",
      "  ghcr.io/ggerganov/llama.cpp:server \\\n",
      "  -m /models/model.gguf)\n",
      "\n",
      "while ! docker logs $container_id 2>&1 | grep -q \"GET /health 127.0.0.1 200\"; do\n",
      "  echo \"Loading model and starting server...\"\n",
      "  docker logs --tail 10 $container_id\n",
      "  sleep 1\n",
      "done\n",
      "echo \"LlamaCpp server is ready!\"\n",
      "\n",
      "\u001b[0mllama_model_loader: - kv  28:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   61 tensors\n",
      "llama_model_loader: - type q5_0:  166 tensors\n",
      "llama_model_loader: - type q8_0:   15 tensors\n",
      "llama_model_loader: - type q4_K:   16 tensors\n",
      "llama_model_loader: - type q6_K:   14 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 98.87 MiB (6.17 BPW) \n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      " \u001b[0m \n",
      "May 7, 2025 > 12:22:29 |  serve._cli.task : 35 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Starting LlamaCpp server and waiting for it to be ready...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "LlamaCpp server is ready!\n",
      " \u001b[0m \n",
      "May 7, 2025 > 12:22:29 |  serve.servers.llamacpp.serve : 145 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Started server for model ID: rag_model \u001b[0m \n",
      "May 7, 2025 > 12:22:29 |  serve.servers.llamacpp.serve : 94 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Added new server instance with model ID: rag_model \u001b[0m \n",
      "May 7, 2025 > 12:22:29 |  serve.experiment_tracker.mlflow.mlflow_llamacpp.manager : 202 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Started serving model rag_model on ports 8000/8080 \u001b[0m \n"
     ]
    }
   ],
   "source": [
    "# model_manager.add_model(\"rag_model\", alias=\"champion\", artifact_path=\"model_path\")\n",
    "model_manager.add_serve(\"rag_model\", server_port=8000, ui_port=8080, gguf_relative_path=\"model_path/artifacts/model.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e1aca91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May 7, 2025 > 12:28:16 |  serve._cli.task : 29 |  \u001b[34m\u001b[1m üêû DEBUG  |  ['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/serve/src/serve/servers/llamacpp'), 'delete', 'ALL=True'] \u001b[0m \n",
      "May 7, 2025 > 12:28:16 |  serve._cli.task : 35 |  \u001b[1m ‚ÑπÔ∏è INFO  |  bbd3a16060a6\n",
      "bbd3a16060a6\n",
      " \u001b[0m \n"
     ]
    }
   ],
   "source": [
    "model_manager.delete_all_serve_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fcdf72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May 7, 2025 > 11:36:48 |  serve._cli.task : 29 |  \u001b[34m\u001b[1m üêû DEBUG  |  ['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/serve/src/serve/servers/llamacpp'), 'delete', 'MODEL_ID=rag_model'] \u001b[0m \n",
      "May 7, 2025 > 11:36:48 |  serve._cli.task : 35 |  \u001b[1m ‚ÑπÔ∏è INFO  |  lmorbits-llamacpp-rag_model\n",
      "lmorbits-llamacpp-rag_model\n",
      " \u001b[0m \n",
      "May 7, 2025 > 11:36:48 |  serve.servers.llamacpp.serve : 220 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Deleted server instance with model ID: rag_model \u001b[0m \n",
      "May 7, 2025 > 11:36:48 |  serve.experiment_tracker.mlflow.mlflow_llamacpp.manager : 234 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Deleted model rag_model \u001b[0m \n",
      "May 7, 2025 > 11:36:48 |  serve._cli.task : 29 |  \u001b[34m\u001b[1m üêû DEBUG  |  ['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/serve/src/serve/servers/llamacpp'), 'status', 'MODEL_ID=rag_model', 'ALL=False'] \u001b[0m \n",
      "May 7, 2025 > 11:36:48 |  serve._cli.task : 33 |  \u001b[31m\u001b[1m ‚ùå ERROR  |  \u001b[31mtask: Failed to run task \"status\": exit status 1\n",
      "\u001b[0m \u001b[0m \n",
      "May 7, 2025 > 11:36:48 |  serve.experiment_tracker.mlflow.mlflow_llamacpp.manager : 168 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Updating model rag_model \u001b[0m \n",
      "May 7, 2025 > 11:36:48 |  serve._cli.task : 29 |  \u001b[34m\u001b[1m üêû DEBUG  |  ['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/serve/src/serve/servers/llamacpp'), 'delete', 'MODEL_ID=rag_model'] \u001b[0m \n",
      "May 7, 2025 > 11:36:48 |  serve._cli.task : 33 |  \u001b[31m\u001b[1m ‚ùå ERROR  |  Error response from daemon: No such container: lmorbits-llamacpp-rag_model\n",
      "\u001b[31mtask: Failed to run task \"delete\": exit status 1\n",
      "\u001b[0m \u001b[0m \n",
      "May 7, 2025 > 11:36:48 |  serve.servers.llamacpp.serve : 115 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Deleted server instance with model ID: rag_model \u001b[0m \n",
      "May 7, 2025 > 11:36:48 |  serve._cli.task : 29 |  \u001b[34m\u001b[1m üêû DEBUG  |  ['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/serve/src/serve/servers/llamacpp'), 'serve', 'MODEL_PATH=/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/model_dir/models/rag_model/model_path/artifacts', 'SERVER_PORTS=7000', 'UI_PORT=8080', 'MODEL_NAME=model.gguf', 'MODEL_ID=rag_model'] \u001b[0m \n",
      "May 7, 2025 > 11:37:19 |  serve._cli.task : 33 |  \u001b[31m\u001b[1m ‚ùå ERROR  |  \u001b[32mtask: [serve] echo \"Starting LlamaCpp server and waiting for it to be ready...\"\n",
      "\u001b[0m\u001b[32mtask: [serve] container_id=$(docker run -d \\\n",
      "  --name lmorbits-llamacpp-rag_model \\\n",
      "  -p 7000:8000 \\\n",
      "  -p 8080:8080 \\\n",
      "  -v /Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/notebooks/model_dir/models/rag_model/model_path/artifacts:/models \\\n",
      "  ghcr.io/ggerganov/llama.cpp:server \\\n",
      "  -m /models/model.gguf)\n",
      "\n",
      "while ! docker logs $container_id 2>&1 | grep -q \"GET /health 127.0.0.1 200\"; do\n",
      "  echo \"Loading model and starting server...\"\n",
      "  docker logs --tail 10 $container_id\n",
      "  sleep 1\n",
      "done\n",
      "echo \"LlamaCpp server is ready!\"\n",
      "\n",
      "\u001b[0mllama_model_loader: - kv  28:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  29:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   61 tensors\n",
      "llama_model_loader: - type q5_0:  166 tensors\n",
      "llama_model_loader: - type q8_0:   15 tensors\n",
      "llama_model_loader: - type q4_K:   16 tensors\n",
      "llama_model_loader: - type q6_K:   14 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 98.87 MiB (6.17 BPW) \n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      "<|im_start|>user\n",
      "Hello<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Hi there<|im_end|>\n",
      "<|im_start|>user\n",
      "How are you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "'\n",
      "main: server is listening on http://0.0.0.0:8080 - starting the main loop\n",
      "srv  update_slots: all slots are idle\n",
      " \u001b[0m \n",
      "May 7, 2025 > 11:37:19 |  serve._cli.task : 35 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Starting LlamaCpp server and waiting for it to be ready...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "Loading model and starting server...\n",
      "LlamaCpp server is ready!\n",
      " \u001b[0m \n",
      "May 7, 2025 > 11:37:19 |  serve.servers.llamacpp.serve : 145 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Started server for model ID: rag_model \u001b[0m \n",
      "May 7, 2025 > 11:37:19 |  serve.servers.llamacpp.serve : 94 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Added new server instance with model ID: rag_model \u001b[0m \n",
      "May 7, 2025 > 11:37:19 |  serve.experiment_tracker.mlflow.mlflow_llamacpp.manager : 199 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Started serving model rag_model on ports 7000/8080 \u001b[0m \n"
     ]
    }
   ],
   "source": [
    "model_manager.update_serve(\"rag_model\", server_port=8000, ui_port=8080, gguf_relative_path=\"model_path/artifacts/model.gguf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad90649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May 7, 2025 > 11:08:42 |  serve._cli.task : 29 |  \u001b[34m\u001b[1m üêû DEBUG  |  ['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/.venv/lib/python3.12/site-packages/serve/servers/llamacpp'), 'stop', 'MODEL_ID=rag_model'] \u001b[0m \n",
      "May 7, 2025 > 11:08:43 |  serve._cli.task : 35 |  \u001b[1m ‚ÑπÔ∏è INFO  |  lmorbits-llamacpp-rag_model\n",
      " \u001b[0m \n",
      "May 7, 2025 > 11:08:43 |  serve.servers.llamacpp.serve : 200 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Stopped server instance with model ID: rag_model \u001b[0m \n",
      "May 7, 2025 > 11:08:43 |  serve.experiment_tracker.mlflow.mlflow_llamacpp.manager : 198 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Stopped model rag_model \u001b[0m \n"
     ]
    }
   ],
   "source": [
    "model_manager.stop_serve_model(\"rag_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "176c4162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May 7, 2025 > 11:10:51 |  serve._cli.task : 29 |  \u001b[34m\u001b[1m üêû DEBUG  |  ['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/.venv/lib/python3.12/site-packages/serve/servers/llamacpp'), 'delete', 'MODEL_ID=rag_model'] \u001b[0m \n",
      "May 7, 2025 > 11:10:52 |  serve._cli.task : 35 |  \u001b[1m ‚ÑπÔ∏è INFO  |  lmorbits-llamacpp-rag_model\n",
      "lmorbits-llamacpp-rag_model\n",
      " \u001b[0m \n",
      "May 7, 2025 > 11:10:52 |  serve.servers.llamacpp.serve : 115 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Deleted server instance with model ID: rag_model \u001b[0m \n"
     ]
    }
   ],
   "source": [
    "model_manager.serve_manager.delete_serve(\"rag_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84175211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May 7, 2025 > 11:13:24 |  serve._cli.task : 29 |  \u001b[34m\u001b[1m üêû DEBUG  |  ['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/.venv/lib/python3.12/site-packages/serve/servers/llamacpp'), 'status', 'MODEL_ID=rag_model', 'ALL=False'] \u001b[0m \n",
      "May 7, 2025 > 11:13:24 |  serve._cli.task : 35 |  \u001b[1m ‚ÑπÔ∏è INFO  |  a414e5e1f202   ghcr.io/ggerganov/llama.cpp:server   \"/app/llama-server -‚Ä¶\"   39 seconds ago   Up 38 seconds (healthy)     0.0.0.0:8000->8000/tcp, :::8000->8000/tcp, 0.0.0.0:8080->8080/tcp, :::8080->8080/tcp   lmorbits-llamacpp-rag_model\n",
      " \u001b[0m \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['task', '--dir', PosixPath('/Users/parsa/Desk/projects/university/slmops-project/slmops-thesis/app_projects/app/pyapp/.venv/lib/python3.12/site-packages/serve/servers/llamacpp'), 'status', 'MODEL_ID=rag_model', 'ALL=False'], returncode=0, stdout='a414e5e1f202   ghcr.io/ggerganov/llama.cpp:server   \"/app/llama-server -‚Ä¶\"   39 seconds ago   Up 38 seconds (healthy)     0.0.0.0:8000->8000/tcp, :::8000->8000/tcp, 0.0.0.0:8080->8080/tcp, :::8080->8080/tcp   lmorbits-llamacpp-rag_model\\n', stderr='')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_manager.serve_manager.get_status(\"rag_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9029996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "May 7, 2025 > 11:22:39 |  serve.experiment_tracker.mlflow.mlflow_llamacpp.manager : 127 |  \u001b[1m ‚ÑπÔ∏è INFO  |  Successfully added model rag_model \u001b[0m \n"
     ]
    }
   ],
   "source": [
    " model_manager.add_model(\"rag_model\", alias=\"champion\", artifact_path=\"model_path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b2f227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
