[project]
name = "simple"
version = "0.0.1"
description = ""
author = ""
dependencies = []

[ml]
type = "llm"
provider = "local"

[ml.serve]
port = 8080
model_name = "rag-model"
alias = "champion"
model_dir = "./models"
tracking = "mlflow"
serving_tech = "llamacpp"

[ml.litellm]
model = "openai/custom"
api_key = "none"
api_base = "http://localhost:8080/v1"
temperature = 0.5
max_tokens = 1000
top_p = 1.0
top_k = 50
